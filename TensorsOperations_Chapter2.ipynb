{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorsOperations_Chapter2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/Deep_Learning_Keras/blob/master/TensorsOperations_Chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14IPmZrT25xY",
        "colab_type": "text"
      },
      "source": [
        "**The gears of neural networks: tensor operations**\n",
        "\n",
        "Much as any computer program can be ultimately reduced to a small set of binary\n",
        "operations on binary inputs (AND, OR, NOR, and so on), all transformations learned\n",
        "by deep neural networks can be reduced to a handful of tensor operations applied to\n",
        "tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors,\n",
        "and so on.\n",
        "\n",
        "In our initial example, we were building our network by stacking Dense layers on\n",
        "top of each other. A Keras layer instance looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytpx8qv05hJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.layers.Dense(512, activation='relu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAST09yv4A7V",
        "colab_type": "text"
      },
      "source": [
        "This layer can be interpreted as a function, which takes as input a *2D* tensor and\n",
        "returns another *2D* tensor—a new representation for the input tensor. Specifically, the\n",
        "function is as follows (where *W* is a *2D* tensor and *b* is a vector, both attributes of the\n",
        "layer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpGTsRAL5UQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = relu(dot(W, input) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDq68xcH5WpB",
        "colab_type": "text"
      },
      "source": [
        "Let’s unpack this. We have three tensor operations here: a dot product (dot) between\n",
        "the input tensor and a tensor named W; an addition (+) between the resulting 2D tensor\n",
        "and a vector b; and, finally, a relu operation. relu(x) is max(x, 0)."
      ]
    }
  ]
}